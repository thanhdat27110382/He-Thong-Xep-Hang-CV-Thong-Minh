import os
import joblib
import re
import pandas as pd
from flask import Flask, request, render_template, jsonify
from PyPDF2 import PdfReader
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from nltk.corpus import stopwords
import nltk
import logging

# Thi·∫øt l·∫≠p logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')

# T·∫£i stopwords
nltk.download('stopwords')
stop_words_en = set(stopwords.words('english'))
try:
    stop_words_vi = set(stopwords.words('vietnamese.txt'))
except:
    stop_words_vi = set()
    logging.warning("Kh√¥ng t√¨m th·∫•y stopwords ti·∫øng Vi·ªát, s·ª≠ d·ª•ng t·∫≠p r·ªóng.")

app = Flask(__name__)

# ƒê·ªãnh nghƒ©a ƒë∆∞·ªùng d·∫´n
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_PATH = os.path.join(BASE_DIR, 'model', 'resume_ranking_model.pkl')
UPLOAD_DIR = os.path.join(BASE_DIR, 'uploads')
OUTPUT_PATH = os.path.join(BASE_DIR, '..', 'csvfiles', 'ketquaxephang', 'resume_ranking_results.xlsx')

# T·∫£i m√¥ h√¨nh
try:
    model = joblib.load(MODEL_PATH)
    logging.info(f"ƒê√£ t·∫£i m√¥ h√¨nh t·ª´ {MODEL_PATH}")
except FileNotFoundError:
    logging.error(f"T·ªáp m√¥ h√¨nh {MODEL_PATH} kh√¥ng t·ªìn t·∫°i.")
    model = None

# ƒê·∫£m b·∫£o th∆∞ m·ª•c uploads t·ªìn t·∫°i
if not os.path.exists(UPLOAD_DIR):
    os.makedirs(UPLOAD_DIR)
    logging.info(f"ƒê√£ t·∫°o th∆∞ m·ª•c uploads: {UPLOAD_DIR}")

def clean_text(text):
    """L√†m s·∫°ch vƒÉn b·∫£n."""
    if not isinstance(text, str):
        text = str(text)
    text = text.lower()
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    words = text.split()
    text = ' '.join(word for word in words if word not in stop_words_en and word not in stop_words_vi)
    logging.debug(f"VƒÉn b·∫£n sau khi l√†m s·∫°ch: {text[:100]}...")
    return text

def extract_text_from_resume(resume_path):
    """Tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ PDF resume."""
    text = ""
    try:
        with open(resume_path, "rb") as file:
            reader = PdfReader(file)
            for page in reader.pages:
                page_text = page.extract_text() or ""
                text += page_text + "\n"
        logging.info(f"ƒê√£ tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ {resume_path}, ƒë·ªô d√†i: {len(text)} k√Ω t·ª±")
    except Exception as e:
        logging.error(f"L·ªói khi tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ {resume_path}: {e}")
    cleaned_text = clean_text(text.strip())
    if not cleaned_text:
        logging.warning(f"Kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c vƒÉn b·∫£n t·ª´ {resume_path}")
    return cleaned_text

def calculate_cosine_similarity(resume_text, job_description):
    """T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng cosine."""
    try:
        tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
        texts = [job_description, resume_text]
        tfidf_matrix = tfidf_vectorizer.fit_transform(texts)
        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
        score = similarity * 100
        logging.debug(f"ƒêi·ªÉm cosine similarity: {score}")
        return round(score, 2), 100
    except Exception as e:
        logging.error(f"L·ªói khi t√≠nh cosine similarity: {e}")
        return 0.0, 100

def get_suggestion(score):
    """Tr·∫£ v·ªÅ g·ª£i √Ω d·ª±a tr√™n ƒëi·ªÉm s·ªë."""
    try:
        if score > 80:
            suggestion = "üî• Excellent match! Your resume is well-optimized."
        elif score > 60:
            suggestion = "‚úÖ Good match! Consider adding more relevant keywords."
        else:
            suggestion = "‚ö° Low match! Try improving your skills section and adding industry-specific terms."
        logging.debug(f"G·ª£i √Ω AI cho ƒëi·ªÉm {score}: {suggestion}")
        return suggestion
    except Exception as e:
        logging.error(f"L·ªói khi t·∫°o g·ª£i √Ω: {e}")
        return "N/A"

@app.route('/')
def index():
    return render_template('indexRanking.html', results=None, top_match=None)

@app.route('/upload', methods=['POST'])
def upload():
    logging.info("Nh·∫≠n y√™u c·∫ßu upload")
    if 'resumes' not in request.files or 'job_description' not in request.form:
        logging.error("Thi·∫øu resumes ho·∫∑c job_description")
        return jsonify({"error": "Vui l√≤ng cung c·∫•p √≠t nh·∫•t m·ªôt resume v√† m√¥ t·∫£ c√¥ng vi·ªác."}), 400

    resume_files = request.files.getlist('resumes')
    job_description = clean_text(request.form['job_description'])

    if not resume_files or not job_description:
        logging.error("Resume ho·∫∑c m√¥ t·∫£ c√¥ng vi·ªác kh√¥ng h·ª£p l·ªá")
        return jsonify({"error": "Resume ho·∫∑c m√¥ t·∫£ c√¥ng vi·ªác kh√¥ng h·ª£p l·ªá."}), 400

    # Ki·ªÉm tra t·ªïng k√≠ch th∆∞·ªõc file
    total_size = sum(f.content_length for f in resume_files if f) / (1024 * 1024)
    if total_size > 200:
        logging.error(f"T·ªïng k√≠ch th∆∞·ªõc file {total_size}MB v∆∞·ª£t qu√° 200MB")
        return jsonify({"error": "T·ªïng k√≠ch th∆∞·ªõc file v∆∞·ª£t qu√° 200MB."}), 400

    results = []
    for resume_file in resume_files:
        if resume_file.filename == '' or not resume_file.filename.endswith('.pdf'):
            logging.warning(f"B·ªè qua file kh√¥ng h·ª£p l·ªá: {resume_file.filename}")
            continue

        # L∆∞u resume
        resume_path = os.path.join(UPLOAD_DIR, resume_file.filename)
        resume_file.save(resume_path)
        logging.info(f"ƒê√£ l∆∞u resume: {resume_path}")

        # Tr√≠ch xu·∫•t v√† l√†m s·∫°ch vƒÉn b·∫£n
        resume_text = extract_text_from_resume(resume_path)
        if not resume_text:
            logging.warning(f"Kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c vƒÉn b·∫£n t·ª´ {resume_file.filename}")
            continue

        # T√≠nh ƒëi·ªÉm cosine similarity
        score, max_score = calculate_cosine_similarity(resume_text, job_description)

        # T·∫°o g·ª£i √Ω AI
        suggestion = get_suggestion(score)

        # D·ª± ƒëo√°n danh m·ª•c
        category = "N/A"
        if model:
            try:
                category = model.predict([resume_text])[0]
                logging.info(f"D·ª± ƒëo√°n danh m·ª•c cho {resume_file.filename}: {category}")
            except Exception as e:
                logging.error(f"L·ªói khi d·ª± ƒëo√°n danh m·ª•c cho {resume_file.filename}: {e}")
        else:
            logging.warning("M√¥ h√¨nh kh√¥ng ƒë∆∞·ª£c t·∫£i, s·ª≠ d·ª•ng danh m·ª•c m·∫∑c ƒë·ªãnh: N/A")

        # L∆∞u k·∫øt qu·∫£
        result = {
            'resume_file': resume_file.filename,
            'score': score,
            'suggestion': suggestion,
            'category': category,
            'job_description': job_description[:100]
        }
        results.append(result)
        logging.debug(f"K·∫øt qu·∫£ cho {resume_file.filename}: {result}")

    # S·∫Øp x·∫øp theo ƒëi·ªÉm s·ªë gi·∫£m d·∫ßn
    results = sorted(results, key=lambda x: x['score'], reverse=True)
    logging.info(f"T·ªïng s·ªë resume ƒë∆∞·ª£c x·ª≠ l√Ω: {len(results)}")

    # L∆∞u v√†o Excel
    try:
        result_df = pd.DataFrame(results)
        if os.path.exists(OUTPUT_PATH):
            try:
                existing_df = pd.read_excel(OUTPUT_PATH)
                result_df = pd.concat([existing_df, result_df], ignore_index=True)
            except Exception as e:
                logging.error(f"L·ªói khi ƒë·ªçc file Excel hi·ªán c√≥: {e}")
        result_df.to_excel(OUTPUT_PATH, index=False)
        logging.info(f"ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o {OUTPUT_PATH}")
    except Exception as e:
        logging.error(f"L·ªói khi l∆∞u Excel: {e}")

    # X√°c ƒë·ªãnh top match
    top_match = results[0]['resume_file'] if results else None
    logging.info(f"Top match: {top_match}")

    return render_template('indexRanking.html', results=results, top_match=top_match)

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=True)