{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "becf1327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import re\n",
    "from openpyxl import Workbook\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "057a3aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the input Excel file\n",
    "input_file = \"D:/BaiDoAnChuyenNganh3/Automated-Resume-Ranking-System-main/csvfiles/crawlcv/ResumeIOLinkIT.xlsx\"\n",
    "df = pd.read_excel(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4981f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store results\n",
    "categories = []\n",
    "resumes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce0fa0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text: remove newlines, extra spaces, and special characters\n",
    "def clean_text(text):\n",
    "    # Remove newlines, tabs, and special characters\n",
    "    text = re.sub(r'[\\n\\r\\t]+', ' ', text)\n",
    "    # Remove non-breaking spaces and other special characters\n",
    "    text = re.sub(r'\\xa0', ' ', text)\n",
    "    # Collapse multiple spaces into a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Strip leading/trailing spaces\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec2ee5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to crawl resume content from a single URL\n",
    "def crawl_resume_content(url, category):\n",
    "    try:\n",
    "        # Set up Selenium WebDriver\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "        \n",
    "        # Navigate to the URL\n",
    "        driver.get(url)\n",
    "        time.sleep(3)  # Wait for page to load\n",
    "        \n",
    "        # Navigate the HTML structure as specified\n",
    "        try:\n",
    "            blog_post = driver.find_element(By.CLASS_NAME, \"blog-post--example\")\n",
    "            content = blog_post.find_element(By.CLASS_NAME, \"blog-post__content\")\n",
    "            content_inner = content.find_element(By.CLASS_NAME, \"blog-post__content-inner\")\n",
    "            content_main = content_inner.find_element(By.CLASS_NAME, \"blog-post__content-main\")\n",
    "            content_main_wrapper = content_main.find_element(By.CLASS_NAME, \"blog-post__content-main-wrapper\")\n",
    "            container = content_main_wrapper.find_element(By.CLASS_NAME, \"text-format_example__container\")\n",
    "            entire_cv = container.find_element(By.ID, \"text-format_example_entire_cv\")\n",
    "        except NoSuchElementException as e:\n",
    "            print(f\"Error: Could not navigate HTML structure for {url}: {str(e)}\")\n",
    "            driver.quit()\n",
    "            return \"\"\n",
    "        \n",
    "        # Initialize list to store resume content\n",
    "        resume_content = []\n",
    "        \n",
    "        # Get all section containers\n",
    "        section_containers = entire_cv.find_elements(By.CLASS_NAME, \"text-format_example__section_container\")\n",
    "        \n",
    "        for idx, section in enumerate(section_containers):\n",
    "            # Handle the first section (Name, Title, Contact Info)\n",
    "            if idx == 0:\n",
    "                try:\n",
    "                    # Get section titles (Name and Job Title)\n",
    "                    titles = section.find_elements(By.CLASS_NAME, \"text-format_example__section_title\")\n",
    "                    for title in titles:\n",
    "                        resume_content.append(clean_text(title.text))\n",
    "                    \n",
    "                    # Get contact info\n",
    "                    description = section.find_element(By.CLASS_NAME, \"text-format_example__section_description\")\n",
    "                    resume_content.append(clean_text(description.text))\n",
    "                except NoSuchElementException:\n",
    "                    print(f\"Warning: Could not find name/title/contact info for {url}\")\n",
    "                    continue\n",
    "            \n",
    "            # Handle Profile section\n",
    "            elif idx == 1:\n",
    "                try:\n",
    "                    title = section.find_element(By.CLASS_NAME, \"text-format_example__section_title\")\n",
    "                    resume_content.append(clean_text(title.text))\n",
    "                    \n",
    "                    description = section.find_element(By.CLASS_NAME, \"text-format_example__section_description\")\n",
    "                    profile_content = \"\"\n",
    "                    try:\n",
    "                        # Try to find <p> tag first\n",
    "                        paragraph = description.find_element(By.TAG_NAME, \"p\")\n",
    "                        profile_content = clean_text(paragraph.text)\n",
    "                    except NoSuchElementException:\n",
    "                        try:\n",
    "                            # If <p> not found, try <div> tag\n",
    "                            div = description.find_element(By.TAG_NAME, \"div\")\n",
    "                            profile_content = clean_text(div.text)\n",
    "                        except NoSuchElementException:\n",
    "                            print(f\"Warning: No profile content (p or div) found for {url}\")\n",
    "                    if profile_content:\n",
    "                        resume_content.append(profile_content)\n",
    "                except NoSuchElementException:\n",
    "                    print(f\"Warning: Could not find profile section for {url}\")\n",
    "                    continue\n",
    "            \n",
    "            # Handle Work Experience section\n",
    "            elif idx == 2:\n",
    "                try:\n",
    "                    title = section.find_element(By.CLASS_NAME, \"text-format_example__section_title\")\n",
    "                    resume_content.append(clean_text(title.text))\n",
    "                    \n",
    "                    description = section.find_element(By.CLASS_NAME, \"text-format_example__section_description\")\n",
    "                    \n",
    "                    # Get all date paragraphs and their corresponding lists\n",
    "                    date_paragraphs = description.find_elements(By.CLASS_NAME, \"text-format_example__dates\")\n",
    "                    ul_elements = description.find_elements(By.TAG_NAME, \"ul\")\n",
    "                    \n",
    "                    # Process each date and its corresponding list\n",
    "                    for i, date_p in enumerate(date_paragraphs):\n",
    "                        resume_content.append(clean_text(date_p.text))\n",
    "                        if i < len(ul_elements):\n",
    "                            try:\n",
    "                                list_items = ul_elements[i].find_elements(By.TAG_NAME, \"li\")\n",
    "                                for li in list_items:\n",
    "                                    resume_content.append(clean_text(li.text))\n",
    "                            except NoSuchElementException:\n",
    "                                print(f\"Warning: No list items found for work experience {i+1} in {url}\")\n",
    "                except NoSuchElementException:\n",
    "                    print(f\"Warning: Could not find work experience section for {url}\")\n",
    "                    continue\n",
    "            \n",
    "            # Handle Education section\n",
    "            elif idx == 3:\n",
    "                try:\n",
    "                    title = section.find_element(By.CLASS_NAME, \"text-format_example__section_title\")\n",
    "                    resume_content.append(clean_text(title.text))\n",
    "                    \n",
    "                    description = section.find_element(By.CLASS_NAME, \"text-format_example__section_description\")\n",
    "                    date_paragraphs = description.find_elements(By.CLASS_NAME, \"text-format_example__dates\")\n",
    "                    for date_p in date_paragraphs:\n",
    "                        resume_content.append(clean_text(date_p.text))\n",
    "                except NoSuchElementException:\n",
    "                    print(f\"Warning: Could not find education section for {url}\")\n",
    "                    continue\n",
    "            \n",
    "            # Handle Languages and Skills section\n",
    "            elif idx == 4:\n",
    "                try:\n",
    "                    titles = section.find_elements(By.CLASS_NAME, \"text-format_example__section_title\")\n",
    "                    ul_elements = section.find_elements(By.TAG_NAME, \"ul\")\n",
    "                    \n",
    "                    # Process Languages (if present)\n",
    "                    if len(titles) > 0:\n",
    "                        resume_content.append(clean_text(titles[0].text))  # Languages\n",
    "                        if len(ul_elements) > 0:\n",
    "                            list_items = ul_elements[0].find_elements(By.TAG_NAME, \"li\")\n",
    "                            for li in list_items:\n",
    "                                resume_content.append(clean_text(li.text))\n",
    "                    \n",
    "                    # Process Skills (if present)\n",
    "                    if len(titles) > 1:\n",
    "                        resume_content.append(clean_text(titles[1].text))  # Skills\n",
    "                        if len(ul_elements) > 1:\n",
    "                            list_items = ul_elements[1].find_elements(By.TAG_NAME, \"li\")\n",
    "                            for li in list_items:\n",
    "                                resume_content.append(clean_text(li.text))\n",
    "                except NoSuchElementException:\n",
    "                    print(f\"Warning: Could not find languages/skills section for {url}\")\n",
    "                    continue\n",
    "        \n",
    "        driver.quit()\n",
    "        \n",
    "        # Join all content with a single space\n",
    "        full_content = \" \".join(resume_content)\n",
    "        return full_content\n",
    "    except Exception as e:\n",
    "        print(f\"Error crawling {url}: {str(e)}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9f97680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling resume for Web Developer...\n",
      "Crawling resume for Software Developer...\n",
      "Crawling resume for Programmer...\n",
      "Crawling resume for Data Analyst ...\n",
      "Crawling resume for Data Scientist...\n",
      "Crawling resume for IT Manager...\n",
      "Crawling resume for Software Engineer...\n",
      "Crawling resume for SOC Analyst...\n",
      "Crawling resume for Network Systems Analyst ...\n",
      "Crawling resume for 3D Animator...\n",
      "Crawling resume for Film and Video Editor...\n",
      "Crawling resume for Computer Science...\n",
      "Crawling resume for Google...\n",
      "Crawling resume for Motion Graphics Artist...\n",
      "Crawling resume for IT Project Manager...\n",
      "Crawling resume for IT Director...\n",
      "Crawling resume for Prompt Engineer...\n",
      "Crawling resume for AI Engineer...\n",
      "Crawling resume for DevOps Engineer...\n",
      "Crawling resume for Network Engineer...\n",
      "Crawling resume for Information Technology...\n",
      "Crawling resume for Technical Project Manager...\n",
      "Crawling resume for Scrum Master...\n",
      "Crawling resume for Systems Analyst...\n",
      "Crawling resume for Senior Software Engineer...\n",
      "Crawling resume for IT Specialist...\n",
      "Crawling resume for Cyber Security...\n",
      "Crawling resume for Data Engineer...\n",
      "Crawling resume for SQL Developer...\n",
      "Crawling resume for Web Analyst...\n",
      "Crawling resume for Power BI Developer...\n",
      "Crawling resume for IT Help Desk...\n",
      "Crawling resume for Full Stack Developer...\n",
      "Crawling resume for Front-end Developer...\n",
      "Crawling resume for Engineering Manager...\n",
      "Crawling resume for Entry-level Software Engineer...\n",
      "Crawling resume for DevOps Professional...\n",
      "Crawling resume for Java Developer...\n",
      "Crawling resume for QA Tester...\n",
      "Crawling resume for Machine Learning Engineer...\n",
      "Crawling resume for Tableau Developer...\n",
      "Crawling resume for Salesforce Admin...\n",
      "Crawling resume for Computer Engineering...\n",
      "Crawling resume for Solution Architect...\n",
      "Crawling resume for AWS Data Engineer...\n",
      "Crawling resume for Computer Science Internship...\n",
      "Crawling resume for Python Developer...\n",
      "Crawling resume for Automation Tester...\n",
      "Crawling resume for Software Tester...\n"
     ]
    }
   ],
   "source": [
    "# Iterate through each row in the input file\n",
    "for index, row in df.iterrows():\n",
    "    category = row[\"Category\"]\n",
    "    url = row[\"Resume_link\"]\n",
    "    print(f\"Crawling resume for {category}...\")\n",
    "    \n",
    "    # Crawl resume content\n",
    "    resume_content = crawl_resume_content(url, category)\n",
    "    \n",
    "    # Append to results\n",
    "    categories.append(category)\n",
    "    resumes.append(resume_content)\n",
    "    \n",
    "    # Optional: Small delay to avoid overwhelming the server\n",
    "    time.sleep(1)\n",
    "\n",
    "# Create a new DataFrame with the results\n",
    "result_df = pd.DataFrame({\n",
    "    \"Category\": categories,\n",
    "    \"Resume\": resumes\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbe2d6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out=pd.DataFrame(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fa505ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Resume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Web Developer</td>\n",
       "      <td>Shane Gomez Web Developer shne.gmez127@gmail.c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Software Developer</td>\n",
       "      <td>Jim Ryan Software Developer jimm_ryyn89@gmail....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Programmer</td>\n",
       "      <td>Taylor Cook Programmer tylor_cookk458@gmail.co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Barry Stevens Data Analyst bst_vensxc_88@gmail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Timothy Smith Data Scientist smit_htimthy_11@g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Category                                             Resume\n",
       "0       Web Developer  Shane Gomez Web Developer shne.gmez127@gmail.c...\n",
       "1  Software Developer  Jim Ryan Software Developer jimm_ryyn89@gmail....\n",
       "2          Programmer  Taylor Cook Programmer tylor_cookk458@gmail.co...\n",
       "3       Data Analyst   Barry Stevens Data Analyst bst_vensxc_88@gmail...\n",
       "4      Data Scientist  Timothy Smith Data Scientist smit_htimthy_11@g..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da39740a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling complete. Results saved to Crawled_Resumes.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Save to Excel\n",
    "result_df.to_excel(\"D:/BaiDoAnChuyenNganh3/Automated-Resume-Ranking-System-main/csvfiles/crawlcv/NewITData/finalresumeiolink.xlsx\", index=False)\n",
    "print(\"Crawling complete. Results saved to Crawled_Resumes.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
